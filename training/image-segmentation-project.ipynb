{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":489236,"sourceType":"datasetVersion","datasetId":202982}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Advanced ML pipeline with segmentation_models and Callbacks\n    \n---","metadata":{"id":"kD6lbSNVLszk"}},{"cell_type":"markdown","source":"## In this Session\n* Understand the dataset\n* Create the dataset preprocessing pipeline\n* Understand each step in preprocessing pipeline","metadata":{"id":"CxcJ0zARmGOB"}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{"id":"pHL8t-2oLszp"}},{"cell_type":"code","source":"from matplotlib import pyplot as plt  # Displaying images\nfrom skimage.io import imread         # Read the images\nimport numpy as np                    # Data Handling\nimport datetime                       # Used in Naming\nimport math                           # Math operations\nimport os                             # Directory files\n\n# One Hot Encoded Mask and Dataset Building\nfrom tensorflow.keras.utils import to_categorical, Sequence\nimport tensorflow as tf\nimport keras\n\n# warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"3eItJm1hLszr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the root directory paths for images and masks\nimg_dir = '/kaggle/input/images/render'\nmask_dir = '/kaggle/input/images/clean'\n\n# Sort the files in root directories, Create and Store the complete image and mask paths\nimages = [os.path.join(img_dir, x) for x in sorted(os.listdir(img_dir))]\nmasks = [os.path.join(mask_dir, x) for x in sorted(os.listdir(mask_dir))]\n\n\n# First 8000 images to be used for training\nX_train = images[:8000]\ny_train = masks[:8000]\n\n# Remaining can be used for validation purpose\nX_valid = images[8000:-4]\ny_valid = masks[8000:-4]\n\n# Save some for testing purpose (last 4)\nX_test = images[-4:]\ny_test = masks[-4:]","metadata":{"id":"ofv9tYqEkO5V","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Break Down of Mask Preprocessing\n","metadata":{"id":"atpOmSqxPdFV"}},{"cell_type":"code","source":"# Consider an example in training data to understand this\nmask_sample_path = y_train[1]\nprint(mask_sample_path)","metadata":{"id":"f7DgeUnKPdFW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read the mask as gray scale\nsample_mask_arr = imread(mask_sample_path, as_gray=True)\nprint(sample_mask_arr.shape)","metadata":{"id":"XnOFS5xLPdFY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cropping the mask\nsample_mask_cropped = sample_mask_arr[:480, :480]\nprint(sample_mask_cropped.shape)","metadata":{"id":"y-zXCzZzPdFa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mask with original size and after cropping\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_data = ((sample_mask_arr, sample_mask_cropped),\n            (sample_mask_arr.shape, sample_mask_cropped.shape))\n\nfor (ax, arr, title) in zip(axes,*(plot_data)) :\n    ax.imshow(arr)\n    ax.set_title(title)\n","metadata":{"id":"oH8v3mOkPdFa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the unique values in cropped_mask\nnp.unique(sample_mask_cropped)","metadata":{"id":"K4ljRz9OPdFb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# What happens if you divide them with 0.07?\nnp.unique(sample_mask_cropped)//0.07","metadata":{"id":"oUyUaOovPdFc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We need to convert the values of 3 to 2 and 10 to 3 after floor operation with 0.07\nadjusted_mask = sample_mask_cropped//0.07\nadjusted_mask[adjusted_mask == 3.0] = 2.0\nadjusted_mask[adjusted_mask == 10.0] = 3.0\nprint(np.unique(adjusted_mask))","metadata":{"id":"JZ6ieMSyPdFe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if any changes in the output (the colors are a bit different! as pixel values changed)\nplt.imshow(adjusted_mask)","metadata":{"id":"CdHV7G9EPdFf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get 4 channel one hot encoded mask\nfinal_sample_mask = to_categorical(adjusted_mask,num_classes=4)\nfinal_sample_mask","metadata":{"id":"N5CYTFamPdFg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Excercise to get from one hot to integer encoded mask (post processing)","metadata":{"id":"la5admV2PdFh"}},{"cell_type":"code","source":"final_sample_mask.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"int_encoded_mask = np.argmax(final_sample_mask, axis=-1)\nint_encoded_mask","metadata":{"id":"qu8Z1soEPdFh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(int_encoded_mask)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# A dummy 4 channel array\nnp.random.seed(42)\ndum_arr = np.random.rand(3, 3, 4)\ndum_arr # 1, 3, 3, # 0, 1, 0 # 1, 2, 1\n\n# [1, 3, 3]\n# [0, 1, 0]\n# [1, 2, 2]","metadata":{"id":"gCAyyVWpPdFi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First row has max value of 0.95 on index 1, hence, 1 is returned on (0,0)\n# Similarly, second row has max value of 0.86 on index 3, hence 3 is returned for (0,1)\n# So on ...\nnp.argmax(dum_arr, axis=-1)","metadata":{"id":"2FvgKg7tPdFi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Data Generator to prepare the Dataset","metadata":{"id":"CJWULFjCPdFi"}},{"cell_type":"code","source":"# Dataset Pipeline\nclass LunarDataset(Sequence):\n\n    # Constructor - x_set, y_set, batch_size, dims, classes\n    def __init__(self, x_set, y_set, batch_size, dims, classes):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.img_height, self.img_width = dims\n        self.classes = classes\n\n    # Number of Batches --> total length of images / batch size --> Ceil operation\n    def __len__(self):\n        return math.ceil(len(self.x)/self.batch_size)\n\n    # Fetch the data in batches by using iter and next opertions\n    def __getitem__(self, idx):\n        # Get start and end indexes to create a batch of batch size\n        start_index = idx * self.batch_size\n        end_index = (idx + 1) * self.batch_size\n        '''\n        0*2 : 1*2 --> 0:2 --> 0, 1\n        1*2 : 2*2 --> 2:4 --> 2, 3\n        2*2 : 3*2 --> 4:6 --> 4, 5\n        '''\n\n        # Prepare X and y batches\n        batch_x = self.x[start_index : end_index]\n        batch_y = self.y[start_index : end_index]\n\n        # Empty lists to append preprocessed Images and Masks Array from the for loop\n        xtr = []\n        ytr = []\n\n        # For every  image and mask in one batch do the following preprocessing\n        for idx, (filename_x, filename_y) in enumerate(zip(batch_x, batch_y)):\n\n            # Image preprocessing\n            img = imread(filename_x)[:self.img_height, :self.img_width, :]/255.0\n            img = img.astype(np.float32)\n            xtr.append(img)\n\n            # Mask preprocessing\n            mask = imread(filename_y, as_gray=True)[:self.img_height, :self.img_width]//0.07 # 0, 1, 3, 10\n            mask[mask == 3] = 2\n            mask[mask == 10] = 3\n            mask = to_categorical(mask, num_classes = self.classes)\n            ytr.append(mask)\n\n        # Convert list to arrays ensuring the dtype of mask is also float32\n        xtr = np.array(xtr)\n        ytr = np.array(ytr).astype(np.float32)\n\n        # Return the preprocessed batch of images and respective mask as output\n        return xtr, ytr","metadata":{"id":"8iibzosHkO5Y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parameters\nbatch_size = 16\ndims = (480, 480)\nnum_classes = 4\n\n# Dataset Creation\ntrain_dataset = LunarDataset(X_train, y_train, batch_size, dims, num_classes)\nvalid_dataset = LunarDataset(X_valid, y_valid, batch_size, dims, num_classes)","metadata":{"id":"GeGFjj3cPdFj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Let's visualize our masks","metadata":{"id":"Kk5QMvOYkO5a"}},{"cell_type":"code","source":"# Taking a sample batch from train_dataset\nbatch = next(iter(train_dataset)) # Batch Size, Height, Width, Channels (Images, Masks)\n\n# Check the shape of batch created --> Images and Masks\nprint(batch[0].shape) # 16 images in a batch\nprint(batch[1].shape) # 16 respective masks in a batch","metadata":{"id":"Cg_PyEcmPdFj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the mask\nsample = batch[1][1] # Second in the batch","metadata":{"id":"zWsosUamPdFj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating Subplot for better visualization\nfig, ((a1, a2, a3), (a4, a5, a6)) = plt.subplots(2, 3, figsize = (10, 8))\n\n# For different axes and titles\nfor i, (ax,title) in enumerate(zip((a1, a2, a3, a4, a5, a6),\n                                   ('Original', 'Combined Mask', 'Background', 'Large Rocks', 'Sky', 'Small Rocks'))):\n    if i == 0:\n        ax.imshow(batch[0][1])                    # Second Image in the batch (Original)\n    elif i == 1:\n        ax.imshow(np.argmax(sample, axis=-1))     # Converts One Hot encoded mask to Integer Encoded Mask (single channel)\n    else:\n        ax.imshow(sample[:, :, i-2])              # Channel Wise Output from Mask\n\n    # Set Title and turn off the axis\n    ax.set_title(title, fontsize=15, weight='bold')\n    ax.axis('off')\n\n# Adjust Layout and Display the Subplot\nplt.tight_layout()\nplt.show()","metadata":{"id":"sz--yYp2kO5b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Let's check out some basic steps of transfer learning using a pretrained model (VGG16)\n","metadata":{}},{"cell_type":"markdown","source":"## segmentation_models\n\n**segmentation_models** is a python library with Neural Networks for Image Segmentation based on Keras and TensorFlow.\n\nThe main features of this library are:\n\n* High level API (just two lines of code to create model for segmentation)\n* 4 models architectures for binary and multi-class image segmentation (including legendary Unet)\n* 25 available backbones for each architecture\n* All backbones have pre-trained weights for faster and better convergence\n* Helpful segmentation losses (Jaccard, Dice, Focal) and metrics (IoU, F-score)","metadata":{}},{"cell_type":"code","source":"# run this command to directly install the library in our notebook\n\n!pip install segmentation_models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* Provide environment variable SM_FRAMEWORK=keras / SM_FRAMEWORK=tf.keras before import segmentation_models\n* Change framework sm.set_framework('keras') / sm.set_framework('tf.keras')","metadata":{}},{"cell_type":"code","source":"# By default it tries to import keras, if it is not installed, it will try to start with tensorflow.keras framework\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\" # Set the environment variable SM_FRAMEWORK to \"tf.keras\"\nimport segmentation_models as sm        # It will import without any errors if env variable is set properly\nsm.set_framework('tf.keras')            # Use segmentation_models library and set the framework to TensorFlow's Keras","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the last axis of the tensor as channel axis\ntf.keras.backend.set_image_data_format('channels_last')\n# Explicitly setting this is not always necessary, as 'channels_last' is the default setting","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building our UNet model with segmentation_models","metadata":{}},{"cell_type":"code","source":"# Parameters for UNET using Segmentation Models\nBACKBONE = 'vgg16'\ninput_shape = (480, 480, 3)\nn_classes = 4\nactivation = 'softmax'\n\n\n# Even though backbone doesn't have encoder-decoder structure, it is still able to build it thanks to sm library\nmodel = sm.Unet(backbone_name = BACKBONE,\n               input_shape = input_shape,\n               classes = n_classes,\n               activation = activation,)\n#                 encoder_weights = 'imagenet',\n#                 encoder_freeze = True)\n\n# Check all the different parameters of UNET, try with them\n# Uncomment the above two lines of UNET for transfer learning\n\n# Check the model summary\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model, 'unet_vgg16.png')","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compile the model","metadata":{}},{"cell_type":"markdown","source":"#### IOU SCORE\n\n* The IoU score will be high if there is much overlap between the anticipated and ground truth boxes.\n* In contrast, a low overlap will result in a low IoU score.\n* An IoU score of 1 indicates a perfect match between the projected box and the ground truth box.\n* Whereas a score of 0 means no overlapping between the boxes.\n  \n<img src='https://b2633864.smushcdn.com/2633864/wp-content/uploads/2016/09/iou_equation.png?lossy=2&strip=1&webp=1' width = 50%>","metadata":{}},{"cell_type":"code","source":"\"\"\" Hyperparameters \"\"\"\nlr = 1e-4\nbatch_size = 16\nepochs = 4\n\n# metrics for result validation\nmetrics = [sm.metrics.IOUScore(threshold=0.5)]\n\n# compiling the model --> Try changing the loss function to jacard_loss from sm library and see changes!\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = tf.keras.optimizers.Adam(lr),\n              metrics = metrics)\n\n\n\"\"\" Callbacks \"\"\"\ncallbacks = [\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor = 'val_iou_score',\n            mode = 'max',\n            patience = 2,\n            factor = 0.1,\n            verbose = 1,\n            min_lr = 1e-6\n        ),\n\n        tf.keras.callbacks.EarlyStopping(\n            monitor = 'val_iou_score',\n            patience = 3,\n            mode = 'max',\n            verbose = 1\n        )\n    ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n- model is compiled with **loss**=\"categorical_crossentropy\",  **optimizer**=Adam, **metrics**=iou_score\n\n**Callbacks** is a tool to customize the behavior of a Keras model during training, evaluation, or inference.\n\n**ReduceLROnPlateau:** Reduce learning rate when a metric has stopped improving.\n\n**EarlyStopping:** Stop training when a monitored metric has stopped improving.","metadata":{}},{"cell_type":"markdown","source":"## Train the Model","metadata":{}},{"cell_type":"code","source":"# Fitting the model\nmodel_history = model.fit(train_dataset,\n        validation_data=valid_dataset,\n        epochs=epochs,\n        callbacks=callbacks,\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Prediction\n* We shall also use this code in our API to build our Streamlit webapp using FastAPI.","metadata":{}},{"cell_type":"code","source":"# function to predict result\ndef predict_image(img_path, mask_path, model):\n    ## Basic Setup\n    H = 480\n    W = 480\n    num_classes = 4\n\n    ## Read Images and Preprocess\n    img = imread(img_path)\n    img = img[:W, :H]\n    img = img / 255.0\n    img = img.astype(np.float32)\n\n    ## Read mask and Preprocess\n    mask = imread(mask_path, as_gray = True)\n    mask = mask[:W, :H]\n\n    ## Prediction mask and Postprocess\n    pred_mask = model.predict(np.expand_dims(img, axis=0))\n    pred_mask = np.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[0]\n\n\n    ## Calculating IOU score\n    inter = np.logical_and(mask, pred_mask)\n    union = np.logical_or(mask, pred_mask)\n\n    iou = inter.sum()/union.sum()\n\n    return img, mask, pred_mask, iou","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test[0], y_test[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index_test = 0\nimg_path = X_test[index_test]\nmask_path = y_test[index_test]\n\nimg, mask, pred_mask, iou = predict_image(img_path, mask_path, model)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (15, 10))\n\nax1.set_title(\"Input Image\")\nax1.imshow(img)\n\nax2.set_title(\"True Mask\")\nax2.imshow(mask)\n\nax3.set_title(\"Predicted mask with IOU score %.2f\"%(iou))\nax3.imshow(pred_mask)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the Model\n* Once satisfied with the model performance, you can save the model.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model, save_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the Model\nsave_model(model, 'LunarModel.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test the saved model","metadata":{}},{"cell_type":"code","source":"# Load the Model and test it\nloaded_model = load_model('/kaggle/working/LunarModel.h5')\n_, _, _, iou_score = predict_image(img_path, mask_path, loaded_model)\nprint(iou_score)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## THE END","metadata":{}}]}